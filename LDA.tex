\section{LDA}
The goal of LDA is to provide a probabilistic model for ``documents,'' which consist of ``words'' from different latent ``topics.'' With some simplifying assumptions such as \vocab{exchangeability} of words (and documents within ``corpora''), LDA aims to represent documents as mixtures of topics, which are distributions of words.

\begin{remark}
    LDA is motivated by the de Fitnetti representation theorem, which states that any collection of exchangeable random variables has a representation as a mixture distribution (in general, an infinite mixture).
\end{remark}
\subsection{Setup}
We can represent LDA by the graphical model in \Cref{fig:LDA}. It has the following hierarchical specification:
\begin{itemize}
    \ii We observe a corpus $\mD$ of $D$ documents 
    $\mathbf{w}_1,\mathbf{w}_2, \ldots, \mathbf{w}_D$.
    \ii Each of these documents is a vector of $N$ words $\left\langle w_1, w_2, \ldots, w_N \right\rangle$.
    \ii The words $w_{dn}$ are generated by latent topics $z_{dn}$, which are generated according to a distribution $\theta_d$. We have a Dirichlet prior with parameter $\alpha$ on the distributions $\theta_d$.\footnote{Why Dirichlet? This is for convenience reasons. It is an exponential family, and it is conjugate to the multinomial distribution (which we are using for topics).} Specifically,
    \begin{align*}
        \theta_d &\sim \text{Dirichlet}(\alpha)\\
        z_{dn} \mid \theta_d &\sim \theta_d\\
        w_{dn} \mid z_{dn}&\sim \beta_{z_{dn}}
    \end{align*}
\end{itemize}
There are some nuances about this model. For instance, do we know the dimensionality $K$ of $\theta$ (and thus the number of topics), and if not, how do we do updates? What if $N$ is not fixed?\footnote{Add a prior on $N$, e.g. Poisson.} What shape does $\beta$ take?\footnote{$\beta$ is a $K \times V$ matrix where $V$ is the size of the dictionary.} We can discuss generalizations in more detail, but for now assume that $K$ is known and $N$ is fixed.\footnote{$N$ in particular is easy to deal with, since it is independent of $\alpha, \theta, z$ so its randomness can easily be ignored.}

\begin{figure}[!htbp]
    \centering
    \begin{asy}
        texpreamble("\usepackage{mathpazo}");
        defaultpen(fontsize(15pt));
        size(9cm);
        pair A = (-3,0), T = (-2,0), Z = (-1,0), W = (0,0), B = (0,1);

        D((-2.5,0.5)--(-2.5,-0.5)--(0.5,-0.5)--(0.5,0.5)--cycle, skyBlue+opacity(0.3)+linewidth(1));
        D((-1.3,0.3)--(-1.3,-0.3)--(0.3,-0.3)--(0.3,0.3)--cycle, skyBlue+opacity(0.7)+linewidth(1));
        D(Z--W,rgb(0.39,0.39,0.39),Arrow(6, Relative(0.97)));
        D(T--Z,rgb(0.39,0.39,0.39),Arrow(6, Relative(0.97)));
        D(A--T,rgb(0.39,0.39,0.39),Arrow(6, Relative(0.97)));
        D(B--W,rgb(0.39,0.39,0.39),Arrow(6, Relative(0.97)));

        label("${N}$",(-1.3,0.3),dir(310),fontsize(10pt));
        label("${D}$",(-2.5,0.5),dir(310),fontsize(10pt));
        label("$\alpha$",A,dir(270));
        dot(A);
        label("$\theta_{d}$",T,dir(270));
        dot(T);
        label("$z_{dn}$",Z,dir(270));
        dot(Z);
        label("$w_{dn}$",W,dir(270));
        dot(W,neonMagenta);
        label("$\beta$",B,dir(90));
        dot(B);
    \end{asy}
    \caption{Graphical model representation of LDA.}
    \label{fig:LDA}
\end{figure}

The Dirichlet density may be written as follows:
\begin{align*}
    p(\theta|\alpha) = \frac{\Gamma\left( \sum\limits_{k=1}^{K} \alpha_k \right)}{\prod\limits_{k=1}^{K} \Gamma(\alpha_k)} \left( \prod\limits_{k=1}^{K} \theta_k^{\alpha_k-1} \right).
\end{align*}
Given parameters $\alpha$ and $\beta$, the joint distribution of $\theta, \mathbf{z}, \mathbf{w}$ ($\mathbf{z}, \mathbf{w}$ are $N$-dimensional, representing a document) is given by
\begin{align*}
    p(\theta, \mathbf{z}, \mathbf{w}\mid \alpha, \beta) 
    &= p(\theta\mid \alpha) \cdot p(\mathbf{z} \mid \theta) \cdot p(\mathbf{w} \mid \mathbf{z}, \beta)\\
    &= p(\theta\mid\alpha) \prod\limits_{n=1}^{N} p(z_n\mid \theta)\cdot p(w_n\mid z_n, \beta).
\end{align*}
Then we can retrieve the marginal distribution of $\mathbf{w}$ conditioned on $\alpha, \beta$ by integrating over $\theta$ and summing over $\mathbf{z}$, and thus the probability of a given corpus. We have
\begin{align*}
    p(\mathbf{w}\mid \alpha, \beta)
    &= \int_{\Theta} p(\theta \mid \alpha) \sum\limits_{\mathbf{z}\in \mZ}^{} p(\mathbf{z}\mid\theta) \cdot p(\mathbf{w}\mid \mathbf{z}, \beta) \dd{\theta}\\
    &= \int_{\Theta} p(\theta \mid \alpha) \left( \prod\limits_{n=1}^{N} \sum\limits_{z_n \in \mZ}^{} p(z_n\mid\theta)\cdot p(w_n\mid z_n, \beta) \right) \dd{\theta}.
\end{align*}
{Therefore, we get}
\begin{align*}
    p(\mD \mid \alpha, \beta)
    &= \prod\limits_{d=1}^{D} p(\mathbf{w}_d \mid \alpha, \beta)\\
    &= \prod\limits_{d=1}^{D}\int_{\Theta} p(\theta_d \mid \alpha) \left( \prod\limits_{n=1}^{N} \sum\limits_{z_{dn} \in \mZ}^{} p(z_{dn}\mid\theta_d)\cdot p(w_{dn}\mid z_{dn}, \beta) \right) \dd{\theta_d}.
\end{align*}
\begin{remark}
    It is important to distinguish LDA from a Dirichlet-multinomial clustering model. The key is that LDA uses an \vocab{admixture}, in which a document can be a mixture of topics, whereas a classical clustering model would restrict a document to be associated with a single topic.
\end{remark}
There are other ways to interpret LDA. By marginalizing over the latent topic variable $z$, we can write LDA as a two-level model rather than a three-level one. Indeed, we can generate a document by choosing $\theta \sim \text{Dir}(\alpha)$ and sampling each word from $p(w\mid \theta, \beta)$, which is given by
\begin{align*}
    p(w\mid \theta, \beta) &= \sum\limits_{z\in \mZ}^{} p(w\mid z, \beta) \cdot p(z\mid  \theta)\\
    \implies p(\mathbf{w}\mid \alpha, \beta) &= \int_{\Theta} p(\theta \mid\alpha) \left( \prod\limits_{n=1}^{N} p(w_n\mid \theta, \beta) \right) \dd{\theta}.
\end{align*}
\begin{ques}
    We have established the motivation and setup behind LDA. Now, how do we perform inference and parameter estimation?
\end{ques}

\subsection{Inference}
In order to use LDA, we must be able to obtain the posterior distribution of the latent variables $\theta, \mathbf{z}$. However, this distribution is intractable because we have
\begin{align*}
    p(\theta, \mathbf{z} \mid \mathbf{w}, \alpha, \beta) &=  \frac{p(\theta, \mathbf{z}, \mathbf{w} \mid \alpha, \beta)}{p(\mathbf{w} \mid \alpha, \beta)}
\end{align*}
and computing $p(\mathbf{w}\mid \alpha, \beta)$ is infeasible (NP-hard!) in general.\footnote{Proven NP-hard by Sontag and Roy for $\alpha\ll 1$.} Therefore, we resort to approximation methods to estimate the posterior instead. Prominent ways include Laplace approximation, MCMC, variational inference. For the rest of the section, we will deal with variational inference in particular.

The key idea of variational inference is that we want to approximate the intractable true posterior with a more computationally reasonable distribution. To do this, we consider a simplified graphical model (\Cref{fig:vargm}), which removes the edges between $\theta, \mathbf{z}$ and drops the $\mathbf{w}$ nodes. This then gives us a much simpler posterior
\[
q(\theta, \mathbf{z} \mid \gamma, \varphi) = q(\theta \mid \gamma) \prod\limits_{n=1}^{N} q(z_n \mid \varphi_n).
\]

\begin{figure}[!htbp]
    \centering
    \begin{asy}
        texpreamble("\usepackage{mathpazo}");
        defaultpen(fontsize(15pt));
        size(9cm);
        pair A = (-3,0), T = (-2,0), Z = (-1,0), W = (0,0), B = (0,1);

        D((-3.5,0.5)--(-3.5,-0.5)--(0.5,-0.5)--(0.5,0.5)--cycle, skyBlue+opacity(0.3)+linewidth(1));
        D((-1.3,0.3)--(-1.3,-0.3)--(0.3,-0.3)--(0.3,0.3)--cycle, skyBlue+opacity(0.7)+linewidth(1));
        D(W--Z,rgb(0.39,0.39,0.39),Arrow(6, Relative(0.97)));
        //D(T--Z,rgb(0.39,0.39,0.39),Arrow(6, Relative(0.97)));
        D(A--T,rgb(0.39,0.39,0.39),Arrow(6, Relative(0.97)));
        //D(B--W,rgb(0.39,0.39,0.39),Arrow(6, Relative(0.97)));

        label("${N}$",(-1.3,0.3),dir(310),fontsize(10pt));
        label("${D}$",(-3.5,0.5),dir(310),fontsize(10pt));
        label("$\gamma_d$",A,dir(270));
        dot(A);
        label("$\theta_{d}$",T,dir(270));
        dot(T);
        label("$z_{dn}$",Z,dir(270));
        dot(Z);
        label("$\varphi_{dn}$",W,dir(270));
        dot(W);
    \end{asy}
    \caption{Simplified graphical model with free variational parameters $(\gamma, \varphi)$. $\gamma$ is a Dirichlet parameter, while ${\varphi} = \left\langle \varphi_1, \ldots, \varphi_N \right\rangle$ are multinomial parameters.}
    \label{fig:vargm}
\end{figure}

We then proceed to do a variant of the EM algorithm as follows:
\begin{enumerate}
    \ii We initialize our parameters $\alpha, \beta$.
    \ii
    \vocab{E-step}.

    For each document $\mathbf{w}$, we compute
    \[
    (\gamma^*(\mathbf{w}), \varphi^*(\mathbf{w})) = \argmin_{(\gamma, \varphi)} \KL{q_{\theta, \mathbf{z} \mid \gamma, \varphi}}{p_{\theta, \mathbf{z} \mid \mathbf{w}, \alpha, \beta}},
    \]
    which corresponds to an I-projection of ${p_{\theta, \mathbf{z} \mid \mathbf{w}, \alpha, \beta}}$ onto $\mQ$, the family of distributions $q$.
    This can be done via an iterative fixed-point method. In particular, we have the update equations
    \begin{align*}
        \varphi_{nk} &\propto \beta_{kw_n}\exp\left( \EE_{q_{\theta\mid \gamma}}\left[\log \theta_{k} \midd \gamma\right] \right)\\
        \gamma_k &= \alpha_k + \sum\limits_{n=1}^{N} \varphi_{nk}.
    \end{align*}
    These are obtained by setting the derivative of KL divergence equal to zero, but they have an intuitive interpretation as well. The update for $\varphi_{nk}$ can be thought of in terms of Bayes' Theorem. Indeed, we have
    \begin{align*}
        \phi_{nz_n} = \PP\left( \sfz_n = z_n \midd \mathsf{w}_n = w_n \right)
        &= p(z_n \mid w_n)\\
        &\propto p(w_n \mid z_n) \cdot p(z_n) \\
        &\approx \beta_{z_nw_n} \exp\left( \EE_{q_{\theta, \mathbf{z} \mid \gamma, \varphi}}\left[ \log q(z_n) \midd \gamma, \varphi \right] \right)\\
        &= \beta_{z_nw_n} \exp\left( \EE_{q_{\theta \mid \gamma}}\left[ \log \theta_{z_n} \midd \gamma \right] \right),
    \end{align*}
    giving the first equation. On the other hand, the update for $\gamma_k$ can be thought of as a Dirichlet update with
    \[
    \theta \mid \gamma \sim \text{Dirichlet}\left( \alpha + \sum\limits_{n=1}^{N} \phi_n \right).
    \]
    This gives us \Cref{alg:i_proj}.
    
    \begin{algorithm}
        \caption{Determining $\gamma^*(\mathbf{w}), \varphi^*(\mathbf{w})$.}
    \begin{algorithmic} 
        \STATE $\varphi_{n} \leftarrow $ uniform over $1, \ldots K$ for $n = 1, \ldots, N$
        \STATE $\gamma_k \leftarrow \alpha_k + \frac{N}{K}$ for $k = 1, \ldots, K$
        \REPEAT 
        \FOR{$n = 1, \ldots, N$}
            \FOR{$k=1, \ldots, K$}

            \STATE $\varphi_{nk}^{t+1}\leftarrow \beta_{kw_n} \exp(\Psi(\gamma_i^t))$
            \ENDFOR
            \STATE normalize $\varphi^{t+1}$
        \ENDFOR
        \STATE $\gamma^{t+1} \leftarrow \alpha + \sum\limits_{n=1}^{N} \varphi_n^{t+1}$
        \UNTIL convergence
        \RETURN $\varphi, \gamma$
    \end{algorithmic}
    \label{alg:i_proj}
    \end{algorithm}

    The line
    $\varphi_{nk}^{t+1}\leftarrow \beta_{kw_n} \exp(\Psi(\gamma_i^t))$
    follows from the fact that
    \begin{align*}
        \varphi_{nk} &\propto
        \beta_{kw_n}\exp\left(\EE_{q_{\theta\mid \gamma}}\left[ \log \theta_k \mid \gamma\right] \right)
        \\
        &= 
        \beta_{kw_n}\exp\left( \Psi(\gamma_k) - \Psi\left( \sum\limits_{k=1}^{K} \gamma_k \right) \right)\\
        &\propto 
        \beta_{kw_n}\exp\left( \Psi(\gamma_k)\right).
    \end{align*}
    This completes the E-step.
    \ii \vocab{M-step}.

    We wish to maximize the log-likelihood of the data with respect to the model parameters $\alpha, \beta$, which is
    \[
    \log p(\mathbf{\mD}\mid \alpha, \beta) = \sum\limits_{d=1}^{D} \log p(\mathbf{w}_d \mid \alpha, \beta) = \sum\limits_{d=1}^{D} \sum\limits_{\mathbf{z}}^{}\int_{\Theta}  \log p(\theta, \mathbf{z}, \mathbf{w}_d\mid \alpha, \beta)\dd{\theta} .
    \]
    For a single document, we can compute the joint distribution as
    \begin{align*}
        \log p(\theta, \mathbf{z}, \mathbf{w}\mid \alpha, \beta) &= \log \left( \prod\limits_{d=1}^{D} \left[ p(\theta_d\mid \alpha) \prod\limits_{n=1}^{N} p(z_{dn}\mid \theta_d) \cdot p(w_{dn}\mid z_{dn}, \beta) \right] \right)
        \\
        &= \sum\limits_{d}^{} \log p(\theta_d\mid \alpha)  + \sum\limits_{d, n}^{} \log p(z_{dn} \mid \theta_d)\cdot p(w_{dn}\mid z_{dn}, \beta).
    \end{align*}
    Computing this joint probability is at least tractable (if not, then we have a problem), unlike $p(\mathbf{w}\mid \alpha, \beta)$ as we originally described. Now we want to use our variational approximation by incorporating $q$.
    We see that by Jensen's inequality,
    \begin{align*}
        \log p(\mathbf{w}\mid \alpha, \beta) 
        &= \sum\limits_{\mathbf{z}}^{} \int_{\Theta} \log p(\theta, \mathbf{z}, \mathbf{w} \mid \alpha, \beta) \dd{\theta}\\
        &= \log \EE_{q_{\theta, \mathbf{z} \mid \gamma, \varphi}}\left[ \frac{p(\theta, \mathbf{z}, \mathbf{w} \mid \alpha, \beta)}{q{(\theta, \mathbf{z} \mid \gamma, \varphi)}}  \right]\\
        &\geq
        \EE_{q_{\theta, \mathbf{z} \mid \gamma, \varphi}}\left[ \log \frac{p(\theta, \mathbf{z}, \mathbf{w} \mid \alpha, \beta)}{q{(\theta, \mathbf{z} \mid \gamma, \varphi)}}  \right]\\
        &= 
        \EE_{q_{\theta, \mathbf{z} \mid \gamma, \varphi}}\left[ \log p(\theta, \mathbf{z}, \mathbf{w} \mid \alpha, \beta) \right] + h\left( q_{\theta, \mathbf{z} \mid \gamma, \varphi} \right)\\
        &\defeq \mL(\mathbf{w}; \alpha, \beta, \gamma, \varphi).
    \end{align*}
    We call this lower bound the \vocab{Evidence Lower Bound} or \vocab{ELBO}. The gap between $\mL$ and the true log marginal is
    \[
    \log p(\mathbf{w}\mid \alpha, \beta) - \mL(\mathbf{w}; \alpha, \beta, \gamma, \varphi) = \KL{q_{\theta, \mathbf{z}\mid \gamma, \varphi}}{p_{\theta, \mathbf{z} \mid \mathbf{w}}}.
    \]
    The bound is tight iff our variational approximation $q_{\theta, \mathbf{z} \mid \gamma, \varphi}$ matches the real distribution over a given document $p_{\theta, \mathbf{z} \mid \mathbf{w}}$.

    Accordingly, we update $\alpha$ and $\beta$ to maximize the ELBO.
    \ii We repeat the E-step and M-step until convergence.
\end{enumerate}
One issue is that sometimes rare words are given probability zero in the multinomial parameters $\beta$. We thus want to ``smooth'' these parameters by assigning all relevant words a nonzero probability. We can try to amend this problem by placing a Dirichlet prior on the multinomial parameter $\beta$. This will give us an intractable posterior on $\beta$, so we will need to extend our use of approximation techniques such as VI, for instance by considering the separable density
\[
q(\beta, \theta, \mathbf{z}\mid \lambda, \gamma, \varphi) = \prod\limits_{k=1}^{K} \text{Dir}(\beta_k\mid \lambda_k) \prod\limits_{d=1}^{D} q_d(\theta_d, \mathbf{z}_d \mid \gamma_d, \varphi_d).
\]
Here $q_d$ is the variational distribution defined for LDA above. This gives us the original update equations as well as a new update for $\lambda$.
